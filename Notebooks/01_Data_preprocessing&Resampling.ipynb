{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd550158",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**\n",
    "\n",
    "This notebook performs the preprocessing of molecular data for QSAR modeling. The primary goals are to clean the dataset, remove potentially problematic compounds, and ensure the quality of descriptors for downstream machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d953ab1f-0a86-4206-8a12-c1cb3dd90c80",
   "metadata": {},
   "source": [
    "## **Step 1: Preparation of SMILES Files for QSAR Modeling**\n",
    "\n",
    "The positive dataset consists of antioxidant compounds (**Positive**) sourced from `Antioxidant-Compound-Library-96-well.xlsx`. \n",
    "Since these compounds are also included within the broader `Bioactive-Compound-Library-I-96-well.xlsx`, we define the negative set by subtracting the positive compounds from the total bioactive library.\n",
    "\n",
    "1. Load Excel files containing compound names and SMILES strings.\n",
    "2. Remove duplicate and missing SMILES from each dataset.\n",
    "3. Extract the **negative** set by removing overlapping SMILES with the **positive** set.\n",
    "4. Save the datasets in `.smi` format for subsequent descriptor calculation using PaDEL.\n",
    "\n",
    "### **Input and Output files:**\n",
    "* Input files:\n",
    "    * antioxi_data_817.smi (**Antioxidant: Positive**)\n",
    "    * total_data_9972.smi (**Total bioactive compounds**) \n",
    "* Raw Outputs:\n",
    "    * antioxi_data_802.smi (Positive compound SMILES format)\n",
    "    * only_neg_data_8898.smi (Negative compound SMILES format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a065a87-64cd-4fad-8cd0-92043373dbb2",
   "metadata": {},
   "source": [
    "#### **Converting raw files to smi files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "614d381c-22fe-4bfc-b91e-53bf97341947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Load Raw data\n",
    "# antioxi_raw_data = pd.read_excel(\"20231011-L6500-Antioxidant-Compound-Library-96-well.xlsx\", sheet_name=1, engine=\"openpyxl\")\n",
    "# total_raw_data = pd.read_excel(\"20240327-L1700-Bioactive-Compound-Library-I-96-well.xlsx\", sheet_name=1, engine=\"openpyxl\")\n",
    "\n",
    "# # Preprocess data\n",
    "# antioxi_data = antioxi_raw_data[[\"SMILES\", 'Name']]                                                                                 # n = 817\n",
    "# total_data = total_raw_data[[\"SMILES\", 'Name']]                                                                                     # n = 9972\n",
    "\n",
    "# # Save to SMI file\n",
    "# antioxi_data.to_csv(\"antioxi_data_817.smi\", sep='\\t', header=False, index=False)  \n",
    "# total_data.to_csv(\"total_data_9972.smi\", sep='\\t', header=False, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c16567c-a1c5-4d94-812e-9f94862c3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SMI files\n",
    "df_antioxi = pd.read_csv(\"antioxi_data_817.smi\", sep=\"\\t\", header=None, names=[\"SMILES\", \"Name\"])\n",
    "df_total   = pd.read_csv(\"total_data_9972.smi\", sep=\"\\t\", header=None, names=[\"SMILES\", \"Name\"])\n",
    "\n",
    "# Removal of duplication and NaN values\n",
    "antioxi_data_dup = df_antioxi.drop_duplicates(['SMILES'], keep = 'first', ignore_index = True)                                      # n = 803\n",
    "antioxi_data_nan = antioxi_data_dup.dropna(subset = ['SMILES'], ignore_index = True)                                                # n = 802\n",
    "\n",
    "total_data_dup = df_total.drop_duplicates(['SMILES'], keep = 'first', ignore_index = True)                                          # n = 9701\n",
    "total_data_nan = total_data_dup.dropna(subset = ['SMILES'], ignore_index = True)                                                    # n = 9700\n",
    "\n",
    "# Merge the DataFrames to find the rows in total_data_nan not present in antioxi_data_nan\n",
    "merged_data = pd.merge(total_data_nan, antioxi_data_nan, on=\"SMILES\", how=\"left\", indicator=True)\n",
    "\n",
    "# Filter the merged DataFrame to keep only the rows that are in total_data_nan but not in antioxi_data_nan\n",
    "only_neg_data = merged_data[merged_data['_merge'] == 'left_only']\n",
    "\n",
    "# Drop the indicator column\n",
    "only_neg_data = only_neg_data.drop(columns=['_merge'])\n",
    "\n",
    "# Rename the columns: drop \"Name_y\" and rename \"Name_x\" to \"Name\"\n",
    "only_neg_data = only_neg_data.drop(columns=['Name_y']).rename(columns={'Name_x': 'Name'})\n",
    "\n",
    "# Remove spaces in the \"Name\" column\n",
    "antioxi_data_nan.loc[:, \"Name\"] = antioxi_data_nan[\"Name\"].str.replace(' ', '')\n",
    "# Save to SMI file\n",
    "antioxi_data_nan.to_csv('antioxi_data_802.smi', sep='\\t', index=False, header=False)                                                # n = 802\n",
    "\n",
    "# Remove spaces in the \"Name\" column\n",
    "only_neg_data.loc[:, \"Name\"] = only_neg_data[\"Name\"].str.replace(' ', '')\n",
    "only_neg_data.to_csv(\"only_neg_data_8898.smi\", sep='\\t', header=False, index=False)                                                 # n = 8898"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187d808",
   "metadata": {},
   "source": [
    "## **Step 2: Filter Out Metal-Containing Compounds**\n",
    "\n",
    "Metal-containing compounds are often problematic in QSAR modeling. They can interfere with descriptor calculation tools such as PaDEL, introduce noise into machine learning models, and lead to poor generalization. To address this, we implemented a rule-based filtering strategy that removes any compound whose SMILES string contains known metal ions.\n",
    "\n",
    "A list of common metal elements, including alkali metals, alkaline earth metals, transition metals, and heavy metals, was used as a matching reference. Additionally, placeholder tokens such as `[R]` and SMARTS-related annotations like `;v` were excluded.\n",
    "\n",
    "### **Input and Output files:**\n",
    "* Input files:\n",
    "    * antioxi_data_802.smi\n",
    "    * only_neg_data_8898.smi (Total **Negative** compounds)\n",
    "* Filtered Outputs:\n",
    "    * filtered_antioxi_data_776.smi\n",
    "    * filtered_only_neg_data_8443.smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7815a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter out metal ions\n",
    "def filter_metal_ions(smiles_file, output_file):\n",
    "    # 메탈 이온 목록\n",
    "    metal_ions = ['Li', 'Be', 'Na', 'Mg', 'Al', 'K', 'Ca', 'Sc', 'Ti', 'V', 'Cr', 'Mn', 'Fe', 'Co', 'Ni', 'Cu', 'Zn', \n",
    "                  'Ga', 'Ge', 'Rb', 'Sr', 'Y', 'Zr', 'Nb', 'Mo', 'Tc', 'Ru', 'Rh', 'Pd', 'Ag', 'Cd', 'In', 'Sn', \n",
    "                  'Sb', 'Cs', 'Ba', 'La', 'Ce', 'Pr', 'Nd', 'Pm', 'Sm', 'Eu', 'Gd', 'Tb', 'Dy', 'Ho', 'Er', 'Tm', \n",
    "                  'Yb', 'Lu', 'Hf', 'Ta', 'W', 'Re', 'Os', 'Ir', 'Pt', 'Au', 'Hg', 'Tl', 'Pb', 'Bi', 'Th', 'Pa', 'U','[R]',';v']\n",
    "\n",
    "    with open(smiles_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')                        \n",
    "            if len(parts) == 2:\n",
    "                smile, name = parts\n",
    "                if not any(metal in smile for metal in metal_ions):\n",
    "                    outfile.write(f\"{smile}\\t{name}\\n\")\n",
    "\n",
    "# Filter metal ions from antioxi_data_nan\n",
    "input_file_antioxi = 'antioxi_data_802.smi'\n",
    "output_file_antioxi = 'filtered_antioxi_data_776.smi'                                                                               # n = 776\n",
    "filter_metal_ions(input_file_antioxi, output_file_antioxi)\n",
    "\n",
    "# Filter metal ions from total_data_nan\n",
    "input_file_total = 'only_neg_data_8898.smi'\n",
    "output_file_total = 'filtered_only_neg_data_8443.smi'                                                                               # n = 8443\n",
    "filter_metal_ions(input_file_total, output_file_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808d363",
   "metadata": {},
   "source": [
    "## **Step 3: Calculation of 1,444 2D Molecular Descriptors using PaDELpy**\n",
    "\n",
    "Molecular descriptors were computed using the `PaDELpy` Python wrapper for the PaDEL-Descriptor Java tool. \n",
    "A total of **1,444 2D descriptors** were calculated for each compound.\n",
    "\n",
    "When the dataset was too large to process in a single run, which may cause memory issues or timeouts. \n",
    "To mitigate this, the .smi file was split into smaller chunks (each containing 500 compounds), and descriptors were computed separately for each chunk.\n",
    "\n",
    "1. Calculate 1,444 2D molecular descriptors using PaDELpy\n",
    "2. Remove compounds with duplicate entries and missing descriptor values (NaN).\n",
    "3. Assign binary class labels: '1' for antioxidant (positive), and '0' for non-antioxidant (negative) compounds.\n",
    "\n",
    "### **Input and Output files:**\n",
    "* Input files:\n",
    "    * filtered_antioxi_data_776.smi\n",
    "    * filtered_only_neg_data_8443.smi\n",
    "* Calculated Outputs:\n",
    "    * antioxi_des_776.csv\n",
    "    * negative_des_8443.csv\n",
    "* Processed Outputs:\n",
    "    * antioxidant_des_dupna_727.csv (Positive descriptors with label 1)\n",
    "    * negative_des_dupna_6677.csv   (Negative descriptors with label 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e83c86-0c02-493c-9d2c-e30963b136d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Positive set ###\n",
    "from padelpy import padeldescriptor\n",
    "\n",
    "padeldescriptor(mol_dir='filtered_antioxi_data_776.smi', d_file='antioxi_des_776.csv', \n",
    "                d_2d=True, d_3d=False, detectaromaticity=True, \n",
    "                log=True, removesalt=True, standardizenitro=True, \n",
    "                usefilenameasmolname=False, \n",
    "                retainorder=True, threads=-1, waitingjobs=-1, \n",
    "                maxruntime=10000, maxcpdperfile=0, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371ce96-2628-4558-a894-147f932db126",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Negative set ###\n",
    "from padelpy import padeldescriptor\n",
    "\n",
    "padeldescriptor(mol_dir='filtered_only_neg_data_8443.smi', d_file='negative_des_8443.csv', \n",
    "                d_2d=True, d_3d=False, detectaromaticity=True, \n",
    "                log=True, removesalt=True, standardizenitro=True, \n",
    "                usefilenameasmolname=False, \n",
    "                retainorder=True, threads=-1, waitingjobs=-1, \n",
    "                maxruntime=10000, maxcpdperfile=0, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d590c2c-35b1-40a5-9cd8-e83042b6eca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Negative set V2 (When an error occurs during descriptor calculation due to large data size) ###\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from padelpy import padeldescriptor\n",
    "# import glob\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def split_smi_file(input_file, output_dir, chunk_size):\n",
    "#     with open(input_file, 'r') as file:\n",
    "#         lines = file.readlines()\n",
    "        \n",
    "#     num_chunks = len(lines) // chunk_size + (1 if len(lines) % chunk_size != 0 else 0)\n",
    "    \n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "    \n",
    "#     for i in range(num_chunks):\n",
    "#         chunk_lines = lines[i * chunk_size: (i + 1) * chunk_size]\n",
    "#         chunk_filename = os.path.join(output_dir, f'chunk_{i + 1}.smi')\n",
    "#         with open(chunk_filename, 'w') as chunk_file:\n",
    "#             chunk_file.writelines(chunk_lines)\n",
    "    \n",
    "#     return num_chunks\n",
    "\n",
    "# # Example usage\n",
    "# num_chunks = split_smi_file('filtered_only_neg_data_8443.smi', 'chunks', 500)\n",
    "# print(f'Total {num_chunks} chunks created.')\n",
    "\n",
    "# chunk_files = glob.glob('chunks/*.smi')\n",
    "\n",
    "# for chunk_file in tqdm(chunk_files, desc=\"Calculating descriptors for each chunk\"):\n",
    "#     output_csv = chunk_file.replace('.smi', '.csv')\n",
    "#     padeldescriptor(mol_dir=chunk_file, d_file=output_csv, \n",
    "#                     d_2d=True, d_3d=False, detectaromaticity=True, \n",
    "#                     log=True, removesalt=True, standardizenitro=True, \n",
    "#                     usefilenameasmolname=False, \n",
    "#                     retainorder=True, threads=16, waitingjobs=-1, \n",
    "#                     maxruntime=10000, maxcpdperfile=0, headless=True)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# csv_files = glob.glob('chunks/*.csv')\n",
    "\n",
    "# dataframes = [pd.read_csv(csv_file) for csv_file in csv_files]\n",
    "# merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# merged_df.to_csv('negative_des_8443_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "177bf656-17a7-4263-a7ba-a79d154d1c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data loading & processing ###\n",
    "positive = pd.read_csv(\"antioxi_des_776.csv\", index_col='Name')                                                                        # n =776\n",
    "negative = pd.read_csv(\"negative_des_8443.csv\", index_col = \"Name\")                                                                    # n = 8443\n",
    "\n",
    "neg_df_dup = negative.drop_duplicates(keep = 'first')                                                                                  # n = 8369\n",
    "pos_df_dup = positive.drop_duplicates(keep='first')                                                                                    # n = 751\n",
    "pos_df, neg_df = pos_df_dup.dropna(), neg_df_dup.dropna()                                                                \n",
    "pos_df, neg_df = (pos_df.assign(y_true=1), neg_df.assign(y_true=0))                                                                    # n = 727, 6677       \n",
    "\n",
    "neg_df.to_csv(\"negative_des_dupna_6677.csv\", index=True)\n",
    "pos_df.to_csv(\"antioxidant_des_dupna_727.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b14955d-662b-488b-a298-a8a5374f9e6a",
   "metadata": {},
   "source": [
    "## **Step 4: Train/Test Split (7:3 ratio)**\n",
    "\n",
    "To prepare for model training and evaluation, the dataset was divided into training and test sets using a **7:3 split ratio** for the positive class. \n",
    "The negative class was subsampled to ensure a **balanced test set**.\n",
    "\n",
    "1. **Positive compounds** were randomly split into:\n",
    "   - 70% training set (n = 508)\n",
    "   - 30% test set (n = 219)\n",
    "2. **Negative compounds** were randomly sampled to match the number of positive test samples (n = 219), ensuring a **balanced test set**.\n",
    "3. The remaining negative compounds (n = 6,458) were used in the training set alongside the positive training samples.\n",
    "4. All sets were saved as CSV files with compound names as index and descriptor values plus a binary label (`y_true`).\n",
    "\n",
    "### **Input and Output files:**\n",
    "* Input File:\n",
    "    * antioxidant_des_dupna_727.csv\n",
    "    * negative_des_dupna_6677.csv\n",
    "* Split Outputs:\n",
    "    * ./train/train_pos_508.csv  (n = 508)  \n",
    "    * ./train/train_total_neg_6458.csv\n",
    "    * ./test/test_set_438.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25ba4a9e-a2fc-4b46-b880-af8665d1e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "### train : test = 7: 3 split (pos) ###\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the preprocessed positive and negative descriptor datasets\n",
    "# neg_df = pd.read_csv(\"negative_des_dupna_6677.csv\", index_col='Name')\n",
    "# pos_df = pd.read_csv(\"antioxidant_des_dupna_727.csv\", index_col='Name')\n",
    "\n",
    "# Split the positive set into 70% training and 30% test sets\n",
    "train_pos, test_pos = train_test_split(pos_df, test_size=0.3, random_state=1004)                                                      # train_pos = 508, test_pos = 219\n",
    "\n",
    "# Randomly sample an equal number of negatives to match the positive test set (219 samples)\n",
    "test_neg = neg_df.sample(n=len(test_pos), random_state=1004)                                                                          # test_neg = 219\n",
    "test_set = pd.concat([test_pos, test_neg])                                                                                            # test_set = 438\n",
    "\n",
    "# Exclude selected negative test samples from the negative pool to define the negative training set\n",
    "train_total_neg = neg_df.drop(test_neg.index)                                                                                         # train_neg = 6458 \n",
    "\n",
    "# Save the resulting datasets to CSV files\n",
    "os.makedirs(\"./train\", exist_ok=True)\n",
    "os.makedirs(\"./test\", exist_ok=True)\n",
    "\n",
    "train_pos.to_csv(\"./train/train_pos_508.csv\", index=True)                                                                             \n",
    "train_total_neg.to_csv(\"./train/train_total_neg_6458.csv\", index=True)                                                               \n",
    "#test_pos.to_csv(\"./test/test_pos_219.csv\", index=True)                                                                                   \n",
    "#test_neg.to_csv(\"./test/test_neg_219.csv\", index=True)                                                                               \n",
    "test_set.to_csv(\"./test/test_set_438.csv\", index=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c7490-e634-4d62-984c-cc6d769e7e45",
   "metadata": {},
   "source": [
    "## **Step 5: Descriptor Filtering by Low Variance**\n",
    "\n",
    "\n",
    "To reduce model complexity and eliminate uninformative features, molecular descriptors with low variance across the dataset were removed.  \n",
    "Features that exhibit minimal variation across both positive and negative classes are unlikely to contribute to classification performance, as they do not help differentiate the classes.\n",
    "\n",
    "1. Combine positive and negative training sets vertically into a single DataFrame.\n",
    "2. Remove the `y_true` label column for feature-only variance analysis.\n",
    "3. Apply a variance threshold of **0.05**: descriptors with variance below this value were removed.\n",
    "4. Construct a filtered training set based only on the selected descriptors.\n",
    "5. Split the filtered set back into positive and negative groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d5957fa-93cc-4b33-bc12-f8fc6960302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low variance 제거 후 남은 feature 개수: 893\n",
      "Positive sample shape: (508, 894)\n",
      "Negative sample shape: (6458, 894)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# train_pos = pd.read_csv(\"./train/train_pos_508.csv\", index_col='Name')\n",
    "# pos_df = pd.read_csv(\"antioxidant_des_dupna_727.csv\", index_col='Name')\n",
    "\n",
    "# Concatenate positive and total negative samples vertically\n",
    "train_total_data = pd.concat([train_pos, train_total_neg], axis=0)\n",
    "\n",
    "# Drop the target column 'y_true' to get feature matrix\n",
    "X_train_total_data = train_total_data.drop(columns=['y_true'])\n",
    "y_train_total_data = train_total_data['y_true']\n",
    "\n",
    "# Remove low variance features - VarianceThreshold removes features with variance lower than the specified threshold\n",
    "threshold = 0.05  # Example: Remove features with variance less than 5%\n",
    "selector = VarianceThreshold(threshold=threshold)\n",
    "X_low_variance_removed = selector.fit_transform(X_train_total_data)\n",
    "\n",
    "# Get the names of selected features\n",
    "selected_features = X_train_total_data.columns[selector.get_support()]\n",
    "print(\"Number of features remaining after low variance removal:\", len(selected_features))\n",
    "\n",
    "# Create DataFrame with selected features\n",
    "train_data_selected = pd.DataFrame(X_low_variance_removed, columns=selected_features)\n",
    "train_data_selected['y_true'] = y_train_total_data.reset_index(drop=True)\n",
    "\n",
    "# Split into positive and negative samples based on 'y_true' value\n",
    "train_pos_selected = train_data_selected[train_data_selected['y_true'] == 1].copy()\n",
    "train_neg_selected = train_data_selected[train_data_selected['y_true'] == 0].copy()\n",
    "\n",
    "# Restore the original index from train_pos and train_total_neg\n",
    "train_pos_selected.index = train_pos.index\n",
    "train_neg_selected.index = train_total_neg.index\n",
    "\n",
    "# Print the result\n",
    "print(\"Positive sample shape:\", train_pos_selected.shape)\n",
    "print(\"Negative sample shape:\", train_neg_selected.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8902e8-1487-4503-b979-e04de77a64b9",
   "metadata": {},
   "source": [
    "# **Resampling for class balancing**\n",
    "\n",
    "This section addresses the inherent class imbalance in the training data by applying a resampling strategy. \n",
    "Since the number of negative samples exceeds that of positive samples, we generate multiple balanced training sets by randomly sampling the negative class with replacement to match the size of the positive class.\n",
    "\n",
    "## **Steps:**\n",
    "1. Merge `train_pos_selected` and `train_neg_selected` after descriptor filtering.\n",
    "2. Repeat the following process 300 times:\n",
    "   - Sample 508 negative compounds **with replacement** from `train_neg_selected` using varying random seeds.\n",
    "   - Combine with the 508 positive compounds to form a balanced binary classification dataset.\n",
    "3. Export each combined training set (total 1016 compounds) to the `./train/` directory for subsequent modeling.\n",
    "   \n",
    "### **Output files:**\n",
    "* Resampled Outputs:\n",
    "    * ./train/train_set_(1~300)_1016.csv (1016 X 893)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e6fe11-ff2b-4f24-a43b-82e9e2f0dd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample negative data to match the number of positive samples (with replacement)\n",
    "for i in range(300):\n",
    "    train_neg = train_neg_selected.sample(n=len(train_pos_selected), replace=True, random_state=1004 + i)  # Varying random state per iteration\n",
    "    train_set = pd.concat([train_pos_selected, train_neg])\n",
    "    # train_neg.to_csv(f'./train/train_neg_{i+1}_508.csv', index=True)  # Optional: save each negative set (508 x 300)\n",
    "    train_set.to_csv(f'./train/train_set_{i+1}_1016.csv', index=True)    # Save combined train set (1016 samples x 300 sets)\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27c84e-8e92-414f-81c2-42b01ee77f78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
